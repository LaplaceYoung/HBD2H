import type { Spread, DrawnCard } from '../utils/tarotLogic';
import { generateLLMPrompt } from '../utils/tarotLogic';

// =========================================================================
// ğŸ”® æ”¯æŒçš„ LLM ä¾›åº”å•†é¢„è®¾ (Supported LLM Providers)
// =========================================================================

export interface LLMProvider {
    id: string;
    name: string;
    baseURL: string;
    defaultModel: string;
    models: string[];
    /** Anthropic ä½¿ç”¨ x-api-key + anthropic-version å¤´éƒ¨è€Œé Bearer Token */
    authStyle: 'bearer' | 'anthropic' | 'gemini';
    /** æ„å»ºæœ€ç»ˆè¯·æ±‚ URL æ—¶æ˜¯å¦éœ€è¦è¿½åŠ æ¨¡å‹åç§°ï¼ˆå¦‚ Geminiï¼‰ */
    urlContainsModel?: boolean;
}

export const LLM_PROVIDERS: LLMProvider[] = [
    {
        id: 'openai',
        name: 'OpenAI',
        baseURL: 'https://api.openai.com/v1/chat/completions',
        defaultModel: 'gpt-4o',
        models: ['gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'o1-mini', 'o3-mini'],
        authStyle: 'bearer',
    },
    {
        id: 'anthropic',
        name: 'Anthropic (Claude)',
        baseURL: 'https://api.anthropic.com/v1/messages',
        defaultModel: 'claude-sonnet-4-20250514',
        models: ['claude-sonnet-4-20250514', 'claude-3-5-sonnet-20241022', 'claude-3-haiku-20240307'],
        authStyle: 'anthropic',
    },
    {
        id: 'gemini',
        name: 'Google Gemini',
        baseURL: 'https://generativelanguage.googleapis.com/v1beta/models',
        defaultModel: 'gemini-2.5-flash',
        models: ['gemini-2.5-flash', 'gemini-2.5-pro', 'gemini-2.0-flash'],
        authStyle: 'gemini',
        urlContainsModel: true,
    },
    {
        id: 'deepseek',
        name: 'DeepSeek',
        baseURL: 'https://api.deepseek.com/v1/chat/completions',
        defaultModel: 'deepseek-chat',
        models: ['deepseek-chat', 'deepseek-reasoner'],
        authStyle: 'bearer',
    },
    {
        id: 'siliconflow',
        name: 'ç¡…åŸºæµåŠ¨ (SiliconFlow)',
        baseURL: 'https://api.siliconflow.cn/v1/chat/completions',
        defaultModel: 'deepseek-ai/DeepSeek-V3',
        models: ['deepseek-ai/DeepSeek-V3', 'Qwen/Qwen2.5-72B-Instruct', 'THUDM/glm-4-9b-chat'],
        authStyle: 'bearer',
    },
    {
        id: 'volcengine',
        name: 'ç«å±±å¼•æ“ (Volcengine)',
        baseURL: 'https://ark.cn-beijing.volces.com/api/v3/chat/completions',
        defaultModel: 'doubao-1.5-pro-32k-250115',
        models: ['doubao-1.5-pro-32k-250115', 'doubao-1.5-lite-32k-250115'],
        authStyle: 'bearer',
    },
];

// =========================================================================
// ğŸ—„ï¸ é…ç½®æŒä¹…åŒ– (localStorage)
// =========================================================================

const STORAGE_KEY = 'tarot_llm_config';

export interface LLMConfig {
    providerId: string;
    apiKey: string;
    baseURL: string;
    model: string;
}

/** ä» localStorage åŠ è½½é…ç½® */
export function loadConfig(): LLMConfig {
    try {
        const raw = localStorage.getItem(STORAGE_KEY);
        if (raw) return JSON.parse(raw);
    } catch { /* å¿½ç•¥æ ¼å¼é”™è¯¯ */ }

    // é»˜è®¤é…ç½®ï¼šOpenAIï¼Œæœªå¡«å†™ Key
    const defaultProvider = LLM_PROVIDERS[0];
    return {
        providerId: defaultProvider.id,
        apiKey: '',
        baseURL: defaultProvider.baseURL,
        model: defaultProvider.defaultModel,
    };
}

/** ä¿å­˜é…ç½®åˆ° localStorage */
export function saveConfig(config: LLMConfig): void {
    localStorage.setItem(STORAGE_KEY, JSON.stringify(config));
}

/** æ ¹æ® providerId è·å–ä¾›åº”å•†é¢„è®¾ */
export function getProvider(id: string): LLMProvider {
    return LLM_PROVIDERS.find(p => p.id === id) || LLM_PROVIDERS[0];
}

// =========================================================================
// ğŸŒŒ æ ¸å¿ƒè¯·æ±‚å‡½æ•°
// =========================================================================

const SYSTEM_PROMPT = `ä½ æ˜¯ä¸€ä½ç¥ç§˜ã€æå…·æ´å¯ŸåŠ›çš„æ˜Ÿé™…å¡”ç½—ç‰Œé«˜é˜¶è§£ç›˜å¤§å¸ˆï¼Œæ“…é•¿é€šè¿‡è£æ ¼å¿ƒç†å­¦ä¸å…ƒç´ å˜åŒ–ä¸ºæ±‚é—®è€…ç­”ç–‘è§£æƒ‘ã€‚
ä½ å¿…é¡»ä»¥å åœå¸ˆçš„å£å»ï¼Œå…¨ç¨‹ä½¿ç”¨ç¬¬äºŒäººç§°â€œä½ â€æ¥ç§°å‘¼æ±‚é—®è€…ï¼Œè¿›è¡Œæ²‰æµ¸å¼ã€å¯¹è¯å¼çš„è§£ç›˜ã€‚
ç›´æ¥è¾“å‡ºæœ€ç»ˆçš„å åœå†…å®¹ï¼Œç»ä¸å‘ˆç°ä»»ä½•è®¡ç®—ã€æ¨ç†æ­¥éª¤æˆ–â€œæ€è€ƒè¿‡ç¨‹â€ã€‚
è¿”å›æ ¼å¼å¿…é¡»æ˜¯ä¼˜ç¾çš„ Markdown æ–‡æœ¬ã€‚ä½¿ç”¨ç®€ä½“ä¸­æ–‡å›å¤ã€‚`;

export async function getTarotReading(question: string, spread: Spread, cards: DrawnCard[]): Promise<string> {
    const prompt = generateLLMPrompt(question, spread, cards);
    console.log("=== ç”Ÿæˆçš„å¤§æ¨¡å‹å åœæç¤ºè¯ ===\n", prompt, "\n=============================");

    const config = loadConfig();

    // å¦‚æ²¡æœ‰é…ç½® API Keyï¼Œåˆ™é™çº§ä¸ºå ä½å›å¤
    if (!config.apiKey) {
        return new Promise((resolve) => {
            setTimeout(() => {
                resolve(`
# âœ¨ å‘½è¿ä¹‹è½´çš„è½¬åŠ¨å·²ä¸ºæ‚¨å¼€å¯

> *å½“å‰å¤„äºã€Œé™é»˜è§‚æµ‹ã€æ¨¡å¼ â€” è¯·å‰å¾€å³ä¸Šè§’ âš™ è®¾ç½®é¢æ¿é…ç½®æ‚¨çš„ API Key*

å¾…æ‚¨æ³¨å…¥åŸåŠ›ï¼ˆAPI Keyï¼‰ï¼Œè¿™é‡Œçš„æ–‡å­—å°†è¢«çœŸæ­£çš„é«˜ç»´å¡”ç½—æ„è¯†å–ä»£ã€‚

## ğŸŒŒ æ˜Ÿè¾°å¯†è¯­ (æ•´ä½“å°è±¡)
ç‰Œé¢å±•ç°å‡ºçš„èƒ½é‡æ·±é‚ƒè€Œå……æ»¡å˜åŠ¨ã€‚\n\næ‚¨æŠ½åˆ°çš„ **${cards[0].nameZh}** ${cards[0].isReversed ? 'ï¼ˆé€†ä½ï¼‰' : 'ï¼ˆæ­£ä½ï¼‰'} æš—ç¤ºä½ éœ€è¦æ›´å¤šçš„å‘å†…æ¢å¯»ã€‚

## ğŸ—ï¸ è¿·é›¾æŒ‡å— (ç»¼åˆå»ºè®®)
åœ¨å½“ä¸‹çš„åå­—è·¯å£ï¼Œæ¥çº³ä¸ç¡®å®šæ€§ã€‚
          `.trim());
            }, 1500);
        });
    }

    const provider = getProvider(config.providerId);

    try {
        // æ ¹æ®ä¾›åº”å•†ç±»å‹æ„å»ºä¸åŒçš„è¯·æ±‚
        if (provider.authStyle === 'anthropic') {
            return await callAnthropic(config, prompt);
        } else if (provider.authStyle === 'gemini') {
            return await callGemini(config, prompt);
        } else {
            return await callOpenAICompatible(config, prompt);
        }
    } catch (error) {
        console.error("å åœè§£ç›˜è¯·æ±‚å¤±è´¥:", error);
        throw new Error("å æ˜Ÿå¡”çš„ä¿¡ä½¿é‡åˆ°äº†è¿·é›¾ã€‚è¯·æ£€æŸ¥ API é…ç½®æˆ–ç½‘ç»œè¿æ¥ã€‚");
    }
}

/** æ¸…ç†å¤§æ¨¡å‹è¿”å›çš„å†…å®¹ï¼Œç§»é™¤å¯èƒ½å­˜åœ¨çš„â€œæ€è€ƒè¿‡ç¨‹â€æˆ–å†…éƒ¨æ ‡ç­¾ */
function cleanResponse(content: string): string {
    if (!content) return "";
    // ç§»é™¤ <thought>...</thought> å’Œ <think>...</think> (å¸¸ç”¨äº DeepSeek R1/Distill ç³»åˆ—)
    let cleaned = content
        .replace(/<thought>[\s\S]*?<\/thought>/gi, "")
        .replace(/<think>[\s\S]*?<\/think>/gi, "")
        .replace(/^\s*æ€è€ƒè¿‡ç¨‹ï¼š[\s\S]*?\n\n/gi, "")
        .replace(/^\s*Thought:[\s\S]*?\n\n/gi, "");

    // ç»ˆæé˜²å¾¡æˆªæ–­ï¼šå¾ˆå¤šæ¨ç†æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯æ»¡è¡€ç‰ˆçš„ DeepSeek-R1ï¼‰å¦‚æœä¸å¸¦ <think> æ ‡ç­¾ï¼Œä¼šæŠŠå¤§æ®µæ€è€ƒè¿‡ç¨‹ç›´æ¥æ‰“å°åœ¨æœ€å‰é¢ã€‚
    // æˆ‘ä»¬å¼ºåˆ¶å¯»æ‰¾æ ‡å‡†å åœæ ‡é¢˜çš„èµ·å§‹ç‚¹ï¼Œå¹¶å°†å…¶ä¹‹å‰çš„æ‰€æœ‰å‘“è¯­å…¨éƒ¨æŠ¹é™¤ã€‚
    const magicStartIndex = cleaned.search(/(?:#+|\*\*|ã€)\s*æ˜Ÿè¾°å¯†è¯­/);
    if (magicStartIndex > 0) {
        cleaned = cleaned.substring(magicStartIndex);
    }

    return cleaned.trim();
}

// ---- OpenAI å…¼å®¹æ ¼å¼ï¼ˆOpenAI / DeepSeek / ç¡…åŸºæµåŠ¨ / ç«å±±å¼•æ“ï¼‰ ----
async function callOpenAICompatible(config: LLMConfig, prompt: string): Promise<string> {
    const response = await fetch(config.baseURL, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${config.apiKey}`,
        },
        body: JSON.stringify({
            model: config.model,
            messages: [
                { role: 'system', content: SYSTEM_PROMPT },
                { role: 'user', content: prompt },
            ],
            temperature: 0.7,
            max_tokens: 2000,
        }),
    });
    if (!response.ok) throw new Error(`API é”™è¯¯: ${response.status} ${response.statusText}`);
    const data = await response.json();
    return cleanResponse(data.choices[0].message.content);
}

// ---- Anthropic Claude æ ¼å¼ ----
async function callAnthropic(config: LLMConfig, prompt: string): Promise<string> {
    const response = await fetch(config.baseURL, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'x-api-key': config.apiKey,
            'anthropic-version': '2023-06-01',
        },
        body: JSON.stringify({
            model: config.model,
            max_tokens: 2000,
            system: SYSTEM_PROMPT,
            messages: [
                { role: 'user', content: prompt },
            ],
        }),
    });
    if (!response.ok) throw new Error(`API é”™è¯¯: ${response.status} ${response.statusText}`);
    const data = await response.json();
    return cleanResponse(data.content[0].text);
}

// ---- Google Gemini æ ¼å¼ ----
async function callGemini(config: LLMConfig, prompt: string): Promise<string> {
    const url = `${config.baseURL}/${config.model}:generateContent?key=${config.apiKey}`;
    const response = await fetch(url, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
            system_instruction: { parts: [{ text: SYSTEM_PROMPT }] },
            contents: [
                { role: 'user', parts: [{ text: prompt }] },
            ],
            generationConfig: { temperature: 0.7, maxOutputTokens: 2000 },
        }),
    });
    if (!response.ok) throw new Error(`API é”™è¯¯: ${response.status} ${response.statusText}`);
    const data = await response.json();
    return cleanResponse(data.candidates[0].content.parts[0].text);
}
